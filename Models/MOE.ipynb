{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8443e7d1888e2683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T10:21:52.515070Z",
     "start_time": "2024-04-08T10:21:51.714689Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#for displaying figures in code editor\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:21:52.523479Z",
     "start_time": "2024-04-08T10:21:52.517961Z"
    }
   },
   "id": "6619cf36434639f1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        logits = self.linear(mh_output) # (B,Token,num_experts)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)# Get top-k experts\n",
    "        #indices: for each observation get the highest values (indices of best models) \n",
    "        # (return the two biggest values of the last dim(-1))\n",
    "        zeros = torch.full_like(logits, float('-inf')) #zero(-inf) matrx of shape logits\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)#keep k-values in matrix\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)#convert to probs\n",
    "        return router_output, indices #output router output and indices\n",
    "\n",
    "#Changing the above to accomodate noisy top-k gating\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        #layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "        #Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "        #Adding scaled unit gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:21:52.531836Z",
     "start_time": "2024-04-08T10:21:52.521017Z"
    }
   },
   "id": "d1f5015b28d6c5fc"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([2, 4, 4]),\n tensor([[[0.3192, 0.6808, 0.0000, 0.0000],\n          [0.3447, 0.6553, 0.0000, 0.0000],\n          [0.4407, 0.0000, 0.0000, 0.5593],\n          [0.0000, 0.9703, 0.0000, 0.0297]],\n \n         [[0.1508, 0.8492, 0.0000, 0.0000],\n          [0.3747, 0.6253, 0.0000, 0.0000],\n          [0.2809, 0.0000, 0.7191, 0.0000],\n          [0.8214, 0.0000, 0.1786, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n torch.Size([2, 4, 2]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing this out:\n",
    "num_experts = 4\n",
    "n_embd = 32\n",
    "top_k = 2\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "print(mh_output.shape)\n",
    "top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
    "\n",
    "gating_output, indices = top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices.shape\n",
    "#And it works!!"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:21:52.560946Z",
     "start_time": "2024-04-08T10:21:52.529785Z"
    }
   },
   "id": "e71ccc5465ef0cf6"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Get weights matrix, containing top-k and their indices(matrix) \n",
    "        gating_output, indices = self.router(x) #[B,tokens,num_experts],[B,tokens,top_k]\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Flatten observations and weight matrix\n",
    "        \n",
    "        flat_x = x.view(-1, x.size(-1)) #[B*tokens,d_model]\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1)) #[B*tokens,num_experts]\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k \n",
    "            #(indices == i): convert [B,tokens,top_k] to bool [B,tokens,top_k] if i present  \n",
    "            #(indices == i).any(dim=-1): dim reduction from [B,tokens,top_k] to [B,tokens] #ex i=0: if [1,0] -->[False,True]->[True]\n",
    "                                   #[2,5] -->[False,False]->[False]\n",
    "                                   #[3,0] -->[False,True]->[True]\n",
    "            print(indices)\n",
    "            print(\"--------\")\n",
    "            #indices [B,Tokens,top_k]\n",
    "            #expert_mask:bool:[B,tokens] (true if indice i was one of the top_k values for each obsevation (token))\n",
    "            expert_mask = (indices == i).any(dim=-1) #[B,tokens]\n",
    "            #flat_mask:bool: boolean mask for top_k indices for current expert\n",
    "            flat_mask = expert_mask.view(-1) #bool[B*tokens] #observations where current expert is top-k\n",
    "            print(\"Number of top_k: \",(flat_mask).sum(dim = -1))\n",
    "            top_k_value =   (flat_mask).sum(dim = -1)\n",
    "            \n",
    "            if flat_mask.any(): #if current expert is in top_k for any observation(token)\n",
    "                #Create a mask for the inputs where the current expert is in top_k\n",
    "                #get obsevations for which the current expert was in top-k (best model)             \n",
    "                expert_input = flat_x[flat_mask]             \n",
    "                #pass top_k observations to expert current expert\n",
    "#                print(\"expert input \", expert_input.shape)\n",
    "            expert_output = expert(expert_input) #[top_k observations,d_model]\n",
    "          \n",
    "            # Extract and apply gating scores\n",
    "            #get the observations' top-k weights for the current expert\n",
    "            #flat_gating_output[flat_mask, i] returns current expert's top_k weights\n",
    "            gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1) #[top_k observations,1]\n",
    "            print(\"flat_gating_output\")\n",
    "            #updating weights \n",
    "            print(\"expert_output \", expert_output.shape)\n",
    "            print(\"gating_scores \", gating_scores.shape)\n",
    "            weighted_output = expert_output * gating_scores #[top_k observations,d_model]\n",
    "            print(\"weighted_output\",weighted_output.shape)\n",
    "            # Update final output additively by indexing and adding\n",
    "            final_output[expert_mask] += weighted_output.squeeze(1) #[top_k observations,d_model]\n",
    "            \n",
    "            print(\"final_output \", final_output.shape)\n",
    "        return final_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:31:22.458302Z",
     "start_time": "2024-04-08T10:31:22.452413Z"
    }
   },
   "id": "4494eb6243b1e706"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# class SparseMoE(nn.Module):\n",
    "#     def __init__(self, n_embed, num_experts, top_k):\n",
    "#         super(SparseMoE, self).__init__()\n",
    "#         self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "#         self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "#         self.top_k = top_k\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         #Get weights matrix, containing top-k and their indices(matrix) \n",
    "#         gating_output, indices = self.router(x) #[B,tokens,num_experts],[B,tokens,top_k]\n",
    "#         final_output = torch.zeros_like(x)\n",
    "#         final_outputt = 0\n",
    "#         \n",
    "#         # Flatten observations and weight matrix\n",
    "#         \n",
    "#         flat_x = x.view(-1, x.size(-1)) #[B*tokens,d_model]\n",
    "#         flat_gating_output = gating_output.view(-1, gating_output.size(-1)) #[B*tokens,num_experts]\n",
    "#         # Process each expert in parallel\n",
    "#         for i, expert in enumerate(self.experts):\n",
    "#             # Create a mask for the inputs where the current expert is in top-k \n",
    "#             #(indices == i): convert [B,tokens,top_k] to bool [B,tokens,top_k] if i present  \n",
    "#             #(indices == i).any(dim=-1): dim reduction from [B,tokens,top_k] to [B,tokens] #ex i=0: if [1,0] -->[False,True]->[True]\n",
    "#                                    #[2,5] -->[False,False]->[False]\n",
    "#                                    #[3,0] -->[False,True]->[True]\n",
    "#             print(indices)\n",
    "#             print(\"--------\")\n",
    "#             #indices [B,Tokens,top_k]\n",
    "#             #expert_mask:bool:[B,tokens] (true if indice i was one of the top_k values for each obsevation (token))\n",
    "#             expert_mask = (indices == i).any(dim=-1) #[B,tokens]\n",
    "#             #flat_mask:bool: boolean mask for top_k indices for current expert\n",
    "#             #flat_mask = expert_mask.view(-1) #bool[B*tokens] #observations where current expert is top-k\n",
    "#             #print(\"Number of top_k: \",(flat_mask).sum(dim = -1))\n",
    "#             #top_k_value =   (flat_mask).sum(dim = -1)\n",
    "#             \n",
    "#             #if flat_mask.any(): #if current expert is in top_k for any observation(token)\n",
    "#                 # Create a mask for the inputs where the current expert is in top_k\n",
    "#                 #get obsevations for which the current expert was in top-k (best model)             \n",
    "#             #    expert_input = flat_x[flat_mask]             \n",
    "#                 #pass top_k observations to expert current expert\n",
    "#             #print(\"expert input \", expert_input.shape)\n",
    "#             expert_output = expert(flat_x) #[top_k observations,d_model]\n",
    "#           \n",
    "#             # Extract and apply gating scores\n",
    "#             #get the observations' top-k weights for the current expert\n",
    "#             #flat_gating_output[flat_mask, i] returns current expert's top_k weights\n",
    "#             \n",
    "#             gating_scores = flat_gating_output[:, i].unsqueeze(1) #[top_k observations,1]\n",
    "#             #updating weights \n",
    "#             weighted_output = expert_output * gating_scores #[top_k observations,d_model]\n",
    "#             # Update final output additively by indexing and adding\n",
    "#             final_outputt += weighted_output.squeeze(1) #[top_k observations,d_model]\n",
    "#             \n",
    "#             print(\"final_output \", final_output.shape)\n",
    "#         return final_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:31:22.785832Z",
     "start_time": "2024-04-08T10:31:22.766478Z"
    }
   },
   "id": "75b76d8d0f040884"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_output torch.Size([4, 8, 16])\n",
      "tensor([[[2, 3],\n",
      "         [0, 2],\n",
      "         [2, 3],\n",
      "         [3, 5],\n",
      "         [5, 1],\n",
      "         [6, 5],\n",
      "         [4, 3],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [1, 4],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [6, 2],\n",
      "         [6, 1],\n",
      "         [1, 0],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [2, 0],\n",
      "         [3, 0],\n",
      "         [0, 1],\n",
      "         [3, 2],\n",
      "         [1, 6],\n",
      "         [4, 6],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [6, 3],\n",
      "         [6, 2],\n",
      "         [6, 3],\n",
      "         [3, 0],\n",
      "         [0, 3],\n",
      "         [1, 3],\n",
      "         [0, 2]]])\n",
      "--------\n",
      "Number of top_k:  tensor(12)\n",
      "flat_gating_output\n",
      "expert_output  torch.Size([12, 16])\n",
      "gating_scores  torch.Size([12, 1])\n",
      "weighted_output torch.Size([12, 16])\n",
      "final_output  torch.Size([4, 8, 16])\n",
      "tensor([[[2, 3],\n",
      "         [0, 2],\n",
      "         [2, 3],\n",
      "         [3, 5],\n",
      "         [5, 1],\n",
      "         [6, 5],\n",
      "         [4, 3],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [1, 4],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [6, 2],\n",
      "         [6, 1],\n",
      "         [1, 0],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [2, 0],\n",
      "         [3, 0],\n",
      "         [0, 1],\n",
      "         [3, 2],\n",
      "         [1, 6],\n",
      "         [4, 6],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [6, 3],\n",
      "         [6, 2],\n",
      "         [6, 3],\n",
      "         [3, 0],\n",
      "         [0, 3],\n",
      "         [1, 3],\n",
      "         [0, 2]]])\n",
      "--------\n",
      "Number of top_k:  tensor(7)\n",
      "flat_gating_output\n",
      "expert_output  torch.Size([7, 16])\n",
      "gating_scores  torch.Size([7, 1])\n",
      "weighted_output torch.Size([7, 16])\n",
      "final_output  torch.Size([4, 8, 16])\n",
      "tensor([[[2, 3],\n",
      "         [0, 2],\n",
      "         [2, 3],\n",
      "         [3, 5],\n",
      "         [5, 1],\n",
      "         [6, 5],\n",
      "         [4, 3],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [1, 4],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [6, 2],\n",
      "         [6, 1],\n",
      "         [1, 0],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [2, 0],\n",
      "         [3, 0],\n",
      "         [0, 1],\n",
      "         [3, 2],\n",
      "         [1, 6],\n",
      "         [4, 6],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [6, 3],\n",
      "         [6, 2],\n",
      "         [6, 3],\n",
      "         [3, 0],\n",
      "         [0, 3],\n",
      "         [1, 3],\n",
      "         [0, 2]]])\n",
      "--------\n",
      "Number of top_k:  tensor(14)\n",
      "flat_gating_output\n",
      "expert_output  torch.Size([14, 16])\n",
      "gating_scores  torch.Size([14, 1])\n",
      "weighted_output torch.Size([14, 16])\n",
      "final_output  torch.Size([4, 8, 16])\n",
      "tensor([[[2, 3],\n",
      "         [0, 2],\n",
      "         [2, 3],\n",
      "         [3, 5],\n",
      "         [5, 1],\n",
      "         [6, 5],\n",
      "         [4, 3],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [1, 4],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [6, 2],\n",
      "         [6, 1],\n",
      "         [1, 0],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [2, 0],\n",
      "         [3, 0],\n",
      "         [0, 1],\n",
      "         [3, 2],\n",
      "         [1, 6],\n",
      "         [4, 6],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [6, 3],\n",
      "         [6, 2],\n",
      "         [6, 3],\n",
      "         [3, 0],\n",
      "         [0, 3],\n",
      "         [1, 3],\n",
      "         [0, 2]]])\n",
      "--------\n",
      "Number of top_k:  tensor(13)\n",
      "flat_gating_output\n",
      "expert_output  torch.Size([13, 16])\n",
      "gating_scores  torch.Size([13, 1])\n",
      "weighted_output torch.Size([13, 16])\n",
      "final_output  torch.Size([4, 8, 16])\n",
      "tensor([[[2, 3],\n",
      "         [0, 2],\n",
      "         [2, 3],\n",
      "         [3, 5],\n",
      "         [5, 1],\n",
      "         [6, 5],\n",
      "         [4, 3],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [1, 4],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [6, 2],\n",
      "         [6, 1],\n",
      "         [1, 0],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [2, 0],\n",
      "         [3, 0],\n",
      "         [0, 1],\n",
      "         [3, 2],\n",
      "         [1, 6],\n",
      "         [4, 6],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [6, 3],\n",
      "         [6, 2],\n",
      "         [6, 3],\n",
      "         [3, 0],\n",
      "         [0, 3],\n",
      "         [1, 3],\n",
      "         [0, 2]]])\n",
      "--------\n",
      "Number of top_k:  tensor(4)\n",
      "flat_gating_output\n",
      "expert_output  torch.Size([4, 16])\n",
      "gating_scores  torch.Size([4, 1])\n",
      "weighted_output torch.Size([4, 16])\n",
      "final_output  torch.Size([4, 8, 16])\n",
      "tensor([[[2, 3],\n",
      "         [0, 2],\n",
      "         [2, 3],\n",
      "         [3, 5],\n",
      "         [5, 1],\n",
      "         [6, 5],\n",
      "         [4, 3],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [1, 4],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [6, 2],\n",
      "         [6, 1],\n",
      "         [1, 0],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [2, 0],\n",
      "         [3, 0],\n",
      "         [0, 1],\n",
      "         [3, 2],\n",
      "         [1, 6],\n",
      "         [4, 6],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [6, 3],\n",
      "         [6, 2],\n",
      "         [6, 3],\n",
      "         [3, 0],\n",
      "         [0, 3],\n",
      "         [1, 3],\n",
      "         [0, 2]]])\n",
      "--------\n",
      "Number of top_k:  tensor(3)\n",
      "flat_gating_output\n",
      "expert_output  torch.Size([3, 16])\n",
      "gating_scores  torch.Size([3, 1])\n",
      "weighted_output torch.Size([3, 16])\n",
      "final_output  torch.Size([4, 8, 16])\n",
      "tensor([[[2, 3],\n",
      "         [0, 2],\n",
      "         [2, 3],\n",
      "         [3, 5],\n",
      "         [5, 1],\n",
      "         [6, 5],\n",
      "         [4, 3],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [1, 4],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [6, 2],\n",
      "         [6, 1],\n",
      "         [1, 0],\n",
      "         [2, 0]],\n",
      "\n",
      "        [[2, 3],\n",
      "         [2, 0],\n",
      "         [3, 0],\n",
      "         [0, 1],\n",
      "         [3, 2],\n",
      "         [1, 6],\n",
      "         [4, 6],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [6, 3],\n",
      "         [6, 2],\n",
      "         [6, 3],\n",
      "         [3, 0],\n",
      "         [0, 3],\n",
      "         [1, 3],\n",
      "         [0, 2]]])\n",
      "--------\n",
      "Number of top_k:  tensor(11)\n",
      "flat_gating_output\n",
      "expert_output  torch.Size([11, 16])\n",
      "gating_scores  torch.Size([11, 1])\n",
      "weighted_output torch.Size([11, 16])\n",
      "final_output  torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "#Let's test this out\n",
    "num_experts = 7\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "dropout=0.1\n",
    "\n",
    "mh_output = torch.randn(4, 8, n_embd)  # Example multi-head attention output\n",
    "print(\"mh_output\", mh_output.shape) #[B,tokens,d_model]\n",
    "\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "final_output = sparse_moe(mh_output)\n",
    "#print(\"Shape of the final output:\", final_output.shape)\n",
    "print(final_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:31:23.117204Z",
     "start_time": "2024-04-08T10:31:23.102938Z"
    }
   },
   "id": "c6fc7d3b7b6efc74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3e3a78a2722c59a2"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
